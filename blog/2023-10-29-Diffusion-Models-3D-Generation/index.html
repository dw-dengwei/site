<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="22-10-06_watson_novel-view-synthesis-with-diffusion-models">22-10-06_Watson_Novel View Synthesis with Diffusion Models</h1> <p>Google Research作品</p> <ul> <li>将输入图片旋转到指定视角</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029160243-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029160243-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029160243-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029160243.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>pose-conditional image-to-image diffusion model</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029160258-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029160258-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029160258-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029160258.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>使用stochastic conditioning鼓励扩散模型生成3D一致的样本</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029164722-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029164722-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029164722-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029164722.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>自回归地生成新视角，conditioning set表示已经有的视角，最开始只有输入图片，随着不断地生成新视角图片，conditioning set逐渐扩增。其中，采样过程中，每一次denoising step都会更换image condition（从conditioning set中随机抽取）</p> <ul> <li>针对该任务对UNet结构重新设计<br> 使用concat的方法将conditioning image融合进模型，效果不好，作者的猜测：只使用self-attention难以学到视角的变换。对于结构的改进见论文，也许后续的文章不再使用这个结构。</li> <li>提出geometry-free新视角生成模型的metric：3D consistency scoring<br> 现有的评价方法无法正确地对三位一致性进行评判，于是提出3D consistency scoring，满足如下要求： <ol> <li>对不满足3D一致性的输出进行惩罚</li> <li> <strong>不</strong>对满足3D一致性但是和GT有出入的输出进行惩罚</li> <li>对不符合输入图片的输出进行惩罚<br> 使用NeRF重建多个新视角图片，具体见原论文<br> 总结：使用pose-guided diffusion model生成新视角，在采样的时候对之前生成的图片都进行考虑，以达到3D一致性</li> </ol> </li> </ul> <h1 id="22-12-06_deng_nerdi-single-view-nerf-synthesis-with-language-guided-diffusion-as-general-image-priors">22-12-06_Deng_NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors</h1> <p>使用SDS优化NeRF有一个问题，预训练模型提供给NeRF的先验不够specific，所以生成出来的三维模型比较平滑，趋向与生成一个比较“平均”的模型</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231030002657-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231030002657-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231030002657-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231030002657.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>所以本文将预训练扩散模型提供的先验分布“缩小”。使用到的扩散模型是text-to-image扩散模型，输入的condition是caption以及使用textual inversion获得的与图片对齐的文本编码。<br> NeRF除了使用SDS进行优化以外，还使用GT单视角图片进行监督。相当于NeRF看到的是输入的单视角图片+先验分布被缩小的扩散模型</p> <h1 id="23-02-15_zhou_sparsefusion-distilling-view-conditioned-diffusion-for-3d-reconstruction">23-02-15_Zhou_SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</h1> <p>少视角NeRF重建<br> 相对于DreamFusion，主要的创新是训练了一个新的扩散模型VLDM</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029205551-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029205551-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029205551-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029205551.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>输入少量视角的图片、对应的相机参数以及目标相机参数，使用EFT将所有condition编码起来</p> <h1 id="23-02-20_gu_nerfdiff-single-image-view-synthesis-with-nerf-guided-distillation-from-3d-aware-diffusion">23-02-20_Gu_NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion</h1> <p>依然是训练一个新的view+image conditioned 扩散模型</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029211217-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029211217-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029211217-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029211217.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>使用UNet对图像进行翻译，转化为三平面，然后在目标视角下采样作为CDM的condition image。两个UNet同时训练：去噪loss+重建loss<br> 扩散模型中用到的Image-conditioned NeRF可以输入图片和相机视角，输出对应视角的图片，但是目前输出的新视角质量较差，于是使用训练完成的扩散模型对这个NeRF进行微调。<br> 有点左脚踩右脚升天的感觉，但是论文效果不错</p> <h1 id="23-03-17_lei_rgbd2-generative-scene-synthesis-via-incremental-view-inpainting-using-rgbd-diffusion-models">23-03-17_Lei_RGBD2: Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029214114-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029214114-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029214114-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029214114.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>输入RGB-D序列（只有少量视角），输出对整个场景重建的mesh<br> 核心思想是利用扩散模型对场景中的孔洞进行补全</p> <h1 id="23-03-20_liu_zero-1-to-3-zero-shot-one-image-to-3d-object">23-03-20_Liu_Zero-1-to-3: Zero-shot One Image to 3D Object</h1> <p>输入图片和目标相机视角，输出符合图片和相机视角的图片，然后可以用这个预训练模型结合SJC生成3D模型<br> 这篇文章相比之前的文章来说，贡献之一是证明了大规模预训练扩散模型有潜力理解图像的三维结构，那么使用in-the-wild 图片也能很好地进行三维重建</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231029151838-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231029151838-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231029151838-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231029151838.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="23-03-30_tseng_consistent-view-synthesis-with-pose-guided-diffusion-models">23-03-30_Tseng_Consistent View Synthesis with Pose-Guided Diffusion Models</h1> <p>同样是训练一个pose-guided diffusion model，输入图片和目标相机参数，输出对应图片。<br> 本文的创新在于提出了一种让UNet理解相机参数的机制，具体来说，提出了一种名为极线注意力机制，利用相机内外参得到的极线约束，对Cross attention产生的注意力分数矩阵进行修改，从而提供让网络能够理解相机视角<br> 文中还交代了一些提高三维一致性的trick</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231030011953-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231030011953-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231030011953-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231030011953.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="23-03-31_xiang_3d-aware-image-generation-using-2d-diffusion-models">23-03-31_Xiang_3D-aware Image Generation using 2D Diffusion Models</h1> <p>和RGBD2 (Lei et. al.) 的方法类似，使用扩散模型预测RGBD，然后手动旋转，将产生的孔洞用扩散模型补全，循环<br> 图形学设计较多</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/assets/img/Pasted%20image%2020231030012551-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/assets/img/Pasted%20image%2020231030012551-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/assets/img/Pasted%20image%2020231030012551-1400.webp"></source> <img src="/assets/assets/img/Pasted%20image%2020231030012551.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="23-04-05_chan_generative-novel-view-synthesis-with-3d-aware-diffusion-models">23-04-05_Chan_Generative Novel View Synthesis with 3D-Aware Diffusion Models</h1> </body></html>