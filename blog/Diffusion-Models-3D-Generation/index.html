<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Diffusion Models - 3D Generation | </title> <meta name="author" content="Wei Deng"> <meta name="description" content="3D generation diffusion model papers"> <meta name="keywords" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dw-dengwei.github.io/blog/Diffusion-Models-3D-Generation/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Diffusion Models - 3D Generation",
      "description": "3D generation diffusion model papers",
      "published": "October 6, 2023",
      "authors": [
        {
          "author": "Wei Deng",
          "authorURL": "https://wdaicv.site",
          "affiliations": [
            {
              "name": "nil",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Diffusion Models - 3D Generation</h1> <p>3D generation diffusion model papers</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#22-10-06-watson-novel-view-synthesis-with-diffusion-models">22-10-06_Watson_Novel View Synthesis with Diffusion Models</a></div> <div><a href="#22-12-06-deng-nerdi-single-view-nerf-synthesis-with-language-guided-diffusion-as-general-image-priors">22-12-06_Deng_NeRDi_Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors</a></div> <div><a href="#23-02-15-zhou-sparsefusion-distilling-view-conditioned-diffusion-for-3d-reconstruction">23-02-15_Zhou_SparseFusion_Distilling View-conditioned Diffusion for 3D Reconstruction</a></div> <div><a href="#23-02-20-gu-nerfdiff-single-image-view-synthesis-with-nerf-guided-distillation-from-3d-aware-diffusion">23-02-20_Gu_NerfDiff_Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion</a></div> <div><a href="#23-03-17-lei-rgbd2-generative-scene-synthesis-via-incremental-view-inpainting-using-rgbd-diffusion-models">23-03-17_Lei_RGBD2_Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models</a></div> <div><a href="#23-03-20-liu-zero-1-to-3-zero-shot-one-image-to-3d-object">23-03-20_Liu_Zero-1-to-3_Zero-shot One Image to 3D Object</a></div> <div><a href="#23-03-30-tseng-consistent-view-synthesis-with-pose-guided-diffusion-models">23-03-30_Tseng_Consistent View Synthesis with Pose-Guided Diffusion Models</a></div> <div><a href="#23-03-31-xiang-3d-aware-image-generation-using-2d-diffusion-models">23-03-31_Xiang_3D-aware Image Generation using 2D Diffusion Models</a></div> <div><a href="#23-04-05-chan-generative-novel-view-synthesis-with-3d-aware-diffusion-models">23-04-05_Chan_Generative Novel View Synthesis with 3D-Aware Diffusion Models</a></div> </nav> </d-contents> <h1 id="22-10-06_watson_novel-view-synthesis-with-diffusion-models">22-10-06_Watson_Novel View Synthesis with Diffusion Models</h1> <p>Google Research作品</p> <ul> <li>将输入图片旋转到指定视角</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029160243-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029160243-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029160243-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029160243.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>pose-conditional image-to-image diffusion model</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029160258-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029160258-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029160258-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029160258.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>使用stochastic conditioning鼓励扩散模型生成3D一致的样本</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029164722-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029164722-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029164722-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029164722.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>自回归地生成新视角，conditioning set表示已经有的视角，最开始只有输入图片，随着不断地生成新视角图片，conditioning set逐渐扩增。其中，采样过程中，每一次denoising step都会更换image condition（从conditioning set中随机抽取）</p> <ul> <li>针对该任务对UNet结构重新设计<br> 使用concat的方法将conditioning image融合进模型，效果不好，作者的猜测：只使用self-attention难以学到视角的变换。对于结构的改进见论文，也许后续的文章不再使用这个结构。</li> <li>提出geometry-free新视角生成模型的metric：3D consistency scoring<br> 现有的评价方法无法正确地对三位一致性进行评判，于是提出3D consistency scoring，满足如下要求： <ol> <li>对不满足3D一致性的输出进行惩罚</li> <li> <strong>不</strong>对满足3D一致性但是和GT有出入的输出进行惩罚</li> <li>对不符合输入图片的输出进行惩罚<br> 使用NeRF重建多个新视角图片，具体见原论文<br> 总结：使用pose-guided diffusion model生成新视角，在采样的时候对之前生成的图片都进行考虑，以达到3D一致性</li> </ol> </li> </ul> <h1 id="22-12-06_deng_nerdi-single-view-nerf-synthesis-with-language-guided-diffusion-as-general-image-priors">22-12-06_Deng_NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors</h1> <p>使用SDS优化NeRF有一个问题，预训练模型提供给NeRF的先验不够specific，所以生成出来的三维模型比较平滑，趋向与生成一个比较“平均”的模型</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231030002657-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231030002657-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231030002657-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231030002657.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>所以本文将预训练扩散模型提供的先验分布“缩小”。使用到的扩散模型是text-to-image扩散模型，输入的condition是caption以及使用textual inversion获得的与图片对齐的文本编码。<br> NeRF除了使用SDS进行优化以外，还使用GT单视角图片进行监督。相当于NeRF看到的是输入的单视角图片+先验分布被缩小的扩散模型</p> <h1 id="23-02-15_zhou_sparsefusion-distilling-view-conditioned-diffusion-for-3d-reconstruction">23-02-15_Zhou_SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction</h1> <p>少视角NeRF重建<br> 相对于DreamFusion，主要的创新是训练了一个新的扩散模型VLDM</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029205551-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029205551-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029205551-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029205551.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>输入少量视角的图片、对应的相机参数以及目标相机参数，使用EFT将所有condition编码起来</p> <h1 id="23-02-20_gu_nerfdiff-single-image-view-synthesis-with-nerf-guided-distillation-from-3d-aware-diffusion">23-02-20_Gu_NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion</h1> <p>依然是训练一个新的view+image conditioned 扩散模型</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029211217-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029211217-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029211217-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029211217.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>使用UNet对图像进行翻译，转化为三平面，然后在目标视角下采样作为CDM的condition image。两个UNet同时训练：去噪loss+重建loss<br> 扩散模型中用到的Image-conditioned NeRF可以输入图片和相机视角，输出对应视角的图片，但是目前输出的新视角质量较差，于是使用训练完成的扩散模型对这个NeRF进行微调。<br> 有点左脚踩右脚升天的感觉，但是论文效果不错</p> <h1 id="23-03-17_lei_rgbd2-generative-scene-synthesis-via-incremental-view-inpainting-using-rgbd-diffusion-models">23-03-17_Lei_RGBD2: Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029214114-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029214114-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029214114-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029214114.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>输入RGB-D序列（只有少量视角），输出对整个场景重建的mesh<br> 核心思想是利用扩散模型对场景中的孔洞进行补全</p> <h1 id="23-03-20_liu_zero-1-to-3-zero-shot-one-image-to-3d-object">23-03-20_Liu_Zero-1-to-3: Zero-shot One Image to 3D Object</h1> <p>输入图片和目标相机视角，输出符合图片和相机视角的图片，然后可以用这个预训练模型结合SJC生成3D模型<br> 这篇文章相比之前的文章来说，贡献之一是证明了大规模预训练扩散模型有潜力理解图像的三维结构，那么使用in-the-wild 图片也能很好地进行三维重建</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231029151838-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231029151838-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231029151838-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231029151838.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="23-03-30_tseng_consistent-view-synthesis-with-pose-guided-diffusion-models">23-03-30_Tseng_Consistent View Synthesis with Pose-Guided Diffusion Models</h1> <p>同样是训练一个pose-guided diffusion model，输入图片和目标相机参数，输出对应图片。<br> 本文的创新在于提出了一种让UNet理解相机参数的机制，具体来说，提出了一种名为极线注意力机制，利用相机内外参得到的极线约束，对Cross attention产生的注意力分数矩阵进行修改，从而提供让网络能够理解相机视角<br> 文中还交代了一些提高三维一致性的trick</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231030011953-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231030011953-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231030011953-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231030011953.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="23-03-31_xiang_3d-aware-image-generation-using-2d-diffusion-models">23-03-31_Xiang_3D-aware Image Generation using 2D Diffusion Models</h1> <p>和RGBD2 (Lei et. al.) 的方法类似，使用扩散模型预测RGBD，然后手动旋转，将产生的孔洞用扩散模型补全，循环<br> 图形学设计较多</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231030012551-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231030012551-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231030012551-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231030012551.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="23-04-05_chan_generative-novel-view-synthesis-with-3d-aware-diffusion-models">23-04-05_Chan_Generative Novel View Synthesis with 3D-Aware Diffusion Models</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231118183023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231118183023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231118183023-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231118183023.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>和其它的新视角生成模型相比，不同的就是condition的方式<br> zero123是将相机参数融合到图像的CLIP embedding里<br> 这个方法是引入了nerf，首先将一个或多个视角的图片编码得到nerf表示，然后进行体渲染得到目标视角的feature作为condition<br> 为什么要引入nerf？TODO</p> <h1 id="23-06-16_dreamsparse-escaping-from-platos-cave-with-2d-frozen-diffusion-model-given-sparse-views">23-06-16_DreamSparse: Escaping from Plato’s Cave with 2D Frozen Diffusion Model Given Sparse Views</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231118193101-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231118193101-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231118193101-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231118193101.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>又提供了一种将目标视角作为condition的方法，目前看不懂</p> <h1 id="23-06-29_one-2-3-45-any-single-image-to-3d-mesh-in-45-seconds-without-per-shape-optimization">23-06-29_One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231118195042-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231118195042-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231118195042-1400.webp"></source> <img src="/assets/img/Pasted%20image%2020231118195042.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>直接使用Zero123生成的多视角图片进行NeRF重建效果不好，是因为Zero123生成的图片具有多视角不一致性<br> 本方法使用Zero123生成的有缺陷的图片作为SparseNeuS的输入进行重建<br> 这个方法没有重新训练或微调一个扩散模型，而是利用了现有的Zero123进行多视角图片的生成3D模型，并且不是基于SDS那样的优化方法，生成速度更快</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"dw-dengwei/site","data-repo-id":"R_kgDOKbiOsg","data-category":"Q&A","data-category-id":"DIC_kwDOKbiOss4CZ54f","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"zh-CN",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Wei Deng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>