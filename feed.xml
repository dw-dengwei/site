<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en, cn"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dw-dengwei.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dw-dengwei.github.io/" rel="alternate" type="text/html" hreflang="en, cn"/><updated>2023-10-06T14:02:21+00:00</updated><id>https://dw-dengwei.github.io/feed.xml</id><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://dw-dengwei.github.io/blog/2023-10-04-Diffusion-Models/" rel="alternate" type="text/html" title=""/><published>2023-10-06T14:02:21+00:00</published><updated>2023-10-06T14:02:21+00:00</updated><id>https://dw-dengwei.github.io/blog/2023-10-04-Diffusion%20Models</id><content type="html" xml:base="https://dw-dengwei.github.io/blog/2023-10-04-Diffusion-Models/"><![CDATA[<p>2023 年扩散模型还有什么可做的方向？ - 谷粒多·凯特的回答 - 知乎 https://www.zhihu.com/question/568791838/answer/3195773725</p> <h1 id="原理篇">原理篇</h1> <h2 id="ddpm">DDPM</h2> <h3 id="前向扩散">前向扩散</h3> <p>前向扩散指的是将一个复杂分布转换成简单分布的过程\(\mathcal{T}:\mathbb{R}^d\mapsto\mathbb{R}^d\)，即：<br/> \(\mathbf{x}_0\sim p_\mathrm{complex}\Longrightarrow \mathcal{T}(\mathbf{x}_0)\sim p_\mathrm{prior}\)<br/> 在DDPM中，将这个过程定义为<strong>马尔可夫链</strong>，通过不断地向复杂分布中的样本\(x_0\sim p_\mathrm{complex}\)添加高斯噪声。这个加噪过程可以表示为\(q(\mathbf{x}_t\vert\mathbf{x}_{t-1})\)：<br/> \(\begin{align} q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) &amp;= \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})\\ \mathbf{x}_t&amp;=\sqrt{1-\beta_t}\mathbf{x}_{t-1}+\sqrt{\beta_t}\boldsymbol\epsilon \quad \boldsymbol\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}) \end{align}\)<br/> 其中，\(\{\beta_t\in(0,1)\}^T_{t=1}\)，是超参数。<br/> 从\(\mathbf{x}_0\)开始，不断地应用\(q(\mathbf{x}_t\vert\mathbf{x}_{t-1})\)，经过足够大的\(T\)步加噪之后，最终得到纯噪声\(\mathbf{x}_T\)：<br/> \(\mathbf{x}_0\sim p_\mathrm{complex}\rightarrow \mathbf{x}_1\rightarrow \cdots \mathbf{x}_t\rightarrow\cdots\rightarrow \mathbf{x}_T\sim p_\mathrm{prior}\)<br/> 除了迭代地使用\(q(\mathbf{x}_t\vert\mathbf{x}_{t-1})\)外，还可以使用\(q(\mathbf{x}_t\vert\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})\)一步到位，证明如下（两个高斯变量的线性组合仍然是高斯变量）：<br/> \(\begin{aligned} \mathbf{x}_t &amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} &amp;\ ;\alpha_t=1-\alpha_t\\ &amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} \\ &amp;= \dots \\ &amp;= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} &amp;\ ;\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0}, \mathbf{I}),\bar{\alpha}_t=\prod_{i=1}^t \alpha_i\ \end{aligned}\)<br/> 一般来说，超参数\(\beta_t\)的设置满足\(0&lt;\beta_1&lt;\cdots&lt;\beta_T&lt;1\)，则\(\bar{\alpha}_1 &gt; \cdots &gt; \bar{\alpha}_T\to1\)，则\(\mathbf{x}_T\)会只保留纯噪声部分。</p> <h3 id="逆向扩散">逆向扩散</h3> <p>在前向扩散过程中，实现了：<br/> \(\mathbf{x}_0\sim p_\mathrm{complex}\rightarrow \mathbf{x}_1\rightarrow \cdots \mathbf{x}_t\rightarrow\cdots\rightarrow \mathbf{x}_T\sim p_\mathrm{prior}\)<br/> 如果能够实现将前向扩散过程反转，也就实现了从简单分布到复杂分布的映射。逆向扩散过程则是将前向过程反转，实现从简单分布随机采样样本，迭代地使用\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)，最终生成复杂分布的样本，即：<br/> \(\mathbf{x}_T\sim p_\mathrm{prior}\rightarrow \mathbf{x}_{T-1}\rightarrow \cdots \mathbf{x}_t\rightarrow\cdots\rightarrow \mathbf{x}_0\sim p_\mathrm{complex}\)<br/> 为了求取\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)，使用贝叶斯公式：<br/> \(\begin{align} q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)&amp;=\frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})q(\mathbf{x}_{t-1})}{q(\mathbf{x}_t)} \end{align}\)<br/> 然而，公式中\(q(x_{t-1})\)和\(q(x_t)\)不好求，根据DDPM的马尔科夫假设，可以为\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)添加条件（可以证明，如果向扩散过程中的\(\beta_t\)足够小，那么\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)是高斯分布。）：<br/> \(\begin{align} q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)&amp;=q(\mathbf{x}_{t-1}\vert\mathbf{x}_t,\mathbf{x}_0)\\ &amp;=\frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1},\mathbf{x}_0)q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)}{q(\mathbf{x}_t\vert\mathbf{x}_0)}\\ &amp;=\frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)}{q(\mathbf{x}_t\vert\mathbf{x}_0)}\\ &amp;=\mathcal{N}(\mathbf{x}_{t-1};\mu(\mathbf{x}_t;\theta),\sigma_t^2\mathbf I) \end{align}\)<br/> 其中，\(\mu(x_t;\theta)\)是高斯分布的均值，\(\sigma_t\)可以用超参数表示：<br/> \(\begin{align} \mu(\mathbf{x}_t;\theta)&amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0\\ \sigma_t&amp;=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t \end{align}\)<br/> 式中\(x_0\)可以反用公式\(\mathbf x_t=\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t\)：<br/> \(\mathbf x_0=\frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t\right)\)<br/> 则：<br/> \(\begin{align} \mu(\mathbf{x}_t;\theta)&amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0\\ &amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}\frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t\right)\\ &amp;=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\boldsymbol\epsilon_t\right) \end{align}\)<br/> 而在推理的时候，\(\boldsymbol\epsilon_t\)是未知的，所以使用神经网络进行预测。综上，逆向扩散过程：<br/> \(\begin{align} q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)&amp;=\mathcal{N}(\mathbf{x}_{t-1};\mu(\mathbf{x}_t;\theta),\sigma_t^2\mathbf I)\\ &amp;=\mathcal{N}\left(\mathbf x_{t-1};\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\boldsymbol\epsilon_\theta(\mathbf x_t, t)\right),\left(\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t\right)^2\mathbf I\right)\\ \mathbf x_{t-1}&amp;=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\boldsymbol\epsilon_\theta(\mathbf x_t, t)\right)+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\cdot\boldsymbol\epsilon\quad\boldsymbol\epsilon\sim\mathcal N(\mathbf 0, \mathbf I) \end{align}\)</p> <h3 id="模型训练">模型训练</h3> <p>DDPM的训练目标是最小化训练数据的负对数似然：<br/> \(\begin{align} -\log p_\theta(\mathbf x_0) &amp;\le -\log p_\theta(\mathbf x_0) + \mathrm{KL}\left(q(\mathbf x_{1:T}\vert\mathbf x_0)\Vert p_\theta(\mathbf x_{1:T}\vert\mathbf x_0)\right) &amp;\ ;\mathrm{KL}(\cdot\Vert\cdot)\ge 0\\ &amp;=-\log p_\theta(\mathbf x_0)+\mathbb{E}_{\mathbf x_{1:T}\sim q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})/p_\theta(\mathbf x_0)}\right]&amp;\ ;p_\theta(\mathbf x_{1:T}\vert\mathbf x_0)=\frac{p_\theta(\mathbf x_{0:T})}{p_\theta(\mathbf x_0)}\\ &amp;=-\log p_\theta(\mathbf x_0)+\mathbb{E}_{\mathbf x_{1:T}\sim q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}+\log p_\theta(\mathbf x_0)\right]\\ &amp;=\mathbb{E}_{\mathbf x_{1:T}\sim q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}\right]\\ \end{align}\)<br/> 其中\(p_\theta(\mathbf x_{1:T}\vert\mathbf x_0)\)是使用网络估计分布\(q\)（变分推断），定义\(\mathcal{L}_{\mathrm{VLB}}\triangleq\mathbb{E}_q(\mathbf x_{0:T})\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}\right]\ge-\mathbb{E}_{q(\mathbf x_0)}\log p_\theta(\mathbf x_0)\)，那么VLB是训练数据的负对数似然的上节，最小化VLB就是最小化负对数似然。继续对VLB拆分：<br/> \(\begin{align} \mathcal{L}_{\mathrm{VLB}}&amp;=\mathbb{E}_{q(\mathbf x_{0:T})}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}\right]\\ &amp;=\mathbb{E}_q\left[\log\frac{\prod_{t=1}^{T}q(\mathbf x_t\vert\mathbf x_{t-1})}{p_\theta(\mathbf x_T)\prod_{t=1}^{T}p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}\right]\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=1}\log\frac{q(\mathbf x_t\vert\mathbf x_{t-1})}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}\right]\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=2}\log\frac{q(\mathbf x_t\vert\mathbf x_{t-1})}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}+\log\frac{q(\mathbf x_1\vert\mathbf x_0)}{p_\theta(\mathbf x_0\vert\mathbf x_1)}\right]\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=2}\log\frac{q(\mathbf x_t\vert\mathbf x_{t-1}, \mathbf x_0)}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}+\log\frac{q(\mathbf x_1\vert\mathbf x_0)}{p_\theta(\mathbf x_0\vert\mathbf x_1)}\right] &amp;\ ;q(\mathbf x_t\vert\mathbf x_{t-1})=q(\mathbf x_t\vert\mathbf x_{t-1}, \mathbf x_0)\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=2}\log\left(\frac{q(\mathbf x_{t-1}\vert\mathbf x_{t}, \mathbf x_0)}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)} \frac{q(\mathbf x_t\vert\mathbf x_0)}{q(\mathbf x_{t-1}\vert\mathbf x_0)}\right)+\log\frac{q(\mathbf x_1\vert\mathbf x_0)}{p_\theta(\mathbf x_0\vert\mathbf x_1)}\right] &amp;\ ;\text{Bayes Theorem}\\ &amp;=\mathbb{E}_q\left[\log\frac{q(\mathbf x_T\vert\mathbf x_0)}{p_\theta(\mathbf x_T)}+\sum_{t=2}^{T}\log\frac{q(\mathbf x_{t-1}\vert\mathbf x_t, \mathbf x_0)}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}-\log p_\theta(\mathbf x_0\vert\mathbf x_1)\right]\\ &amp;=\mathbb{E}_q\left[\underbrace{\mathrm{KL}(q(\mathbf x_T\vert\mathbf x_0) \Vert p_\theta(\mathbf x_T))}_{\mathcal{L}_T} + \sum_{t=2}^{T}\underbrace{\mathrm{KL}(q(\mathbf x_{t-1}\vert\mathbf x_t, \mathbf x_0) \Vert p_\theta(\mathbf x_{t-1}\vert\mathbf x_t))}_{\mathcal{L}_{t-1}}-\underbrace{\log p_\theta(\mathbf x_0\vert\mathbf x_1)}_{\mathcal{L}_0}\right]\\ &amp;=\mathbb{E}_q\left[\mathcal{L}_T+\sum_{t=2}^{T}\mathcal{L}_{t-1}-\mathcal{L}_0\right] \end{align}\)</p> <ol> <li>由于\(\mathbf x_T\)是纯噪声，所以\(\mathcal{L}_T\)是常数</li> <li>对于\(\mathcal{L}_0\)，DDPM专门设计了特殊的\(p_\theta(\mathbf x_0\vert\mathbf x_1)\)</li> <li>对于\(\mathcal{L}_t\triangleq\mathrm{KL}(q(\mathbf x_t\vert\mathbf x_{t+1}, \mathbf x_0) \Vert p_\theta(\mathbf x_t \vert \mathbf x_{t+1})) \quad 1\le t \le T-1\)，是两个正态分布的KL散度，有解析解。在DDPM中，使用了简化之后的损失函数：<br/> \(\begin{align} \mathcal{L}_t^{\mathrm{simple}}&amp;=\mathbb{E}_{t\sim[1,T],\mathbf x_0,\boldsymbol\epsilon_t}\left[\Vert\boldsymbol\epsilon_t-\boldsymbol\epsilon_\theta(\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t,t)\Vert^2_2\right] \end{align}\) <h3 id="总结">总结</h3> <p>综上，DDPM的训练和采样/推理过程如下图所示：</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%20%25%25%2020231002142935%20%25%25-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%20%25%25%2020231002142935%20%25%25-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%20%25%25%2020231002142935%20%25%25-1400.webp"/> <img src="/assets/img/Pasted%20image%20%25%25%2020231002142935%20%25%25.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="基于score的生成模型">基于score的生成模型</h2> <h3 id="引言">引言</h3> <p>score-based生成模型是一种新的生成模型范式，在score-based之前，主要存在两种类型的生成模型：</p> <ol> <li><strong>基于似然的生成模型</strong>：基于最大似然估计（MLE），使得从真实数据分布中采样出的数据能够在所建模的数据分布中概率最大化，即\(\max_\theta \mathbb{E}_{x\sim p_\mathrm{data}(x)}\left[\log p(x;\theta)\right]\)。这类模型通过最大化似然函数，直接学习数据集的分布，主要方法有VAE、流模型、能量模型</li> <li><strong>隐式生成模型</strong>：非直接学习数据集的分布，主要方法有GAN<br/> 它们分别具有以下限制：</li> <li><strong>对于基于似然的生成模型</strong>：对神经网络的结构要求高</li> <li><strong>对于隐式生成模型</strong>：训练不稳定、容易导致模式坍塌 <h3 id="先验知识">先验知识</h3> <p>score-based生成模型就能很好地避免这些限制，在介绍score-based生成模型之前，需要明确几个概念：</p> </li> <li><strong>能量函数</strong><br/> 对于大多数分布，可以使用概率密度函数（PDF）进行描述，也可以使用能量函数进行描述：<br/> \(p(x)=\frac{e^{-E(x)}}{Z}\)<br/> 其中\(p(x)\)是PDF，\(E(x)\)是能量函数，\(Z=\int e^{-E(x)}\mathrm{d}x\)是归一化因子。以高斯分布为例，可以使用以下能量函数进行描述：<br/> \(\begin{align} E(x;\mu,\sigma^2)&amp;=\frac{1}{2\sigma^2}(x-\mu)^2\\ p(x)&amp;=\frac{e^{-E(x)}}{\int e^{-E(x)}\mathrm dx}=\frac{e^{\frac{1}{2\sigma^2}(x-\mu)^2}}{\sqrt{2\pi\sigma^2}} \end{align}\)</li> <li><strong>能量模型</strong><br/> 能量模型是基于能量函数的生成模型：<br/> \(p_\theta(x)=\frac{\exp(-E_\theta(x))}{Z_\theta}\)<br/> 基于能量函数对数据分布进行建模的时候，<strong>如何计算能量模型的归一化函数\(Z_\theta\)是一个较难的问题</strong>。传统的基于似然的生成模型（如自回归模型、流模型、VAE）都有自己的解决方式，但是都对能量模型做了太多的约束，各自都有其限制。</li> <li><strong>蒙特卡罗采样方法：拒绝-接受法</strong><br/> 目的：希望从一个复杂分布\(p(x)\)采样\(N\)个样本<br/> 方法：使用一个简单分布\(q(x)\)为媒介（例如：高斯分布），这个分布必须满足它的\(c&gt;0\)倍大于等于\(p(x)\)。首先从简单分布\(q(x)\)中采样得到\(x^*\)，然后以\(\frac{p(x^*)}{cq(x^*)}\)的概率保留这个样本，直到得到\(N\)个样本结束。</li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231002164638-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231002164638-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231002164638-1400.webp"/> <img src="/assets/img/Pasted%20image%2020231002164638.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li><strong>MCMC</strong><br/> <strong>MCMC方法可以从复杂分布中进行采样</strong><br/> <strong>符号定义</strong>：\(\pi_i\triangleq\pi(i)=\lim_{t\to +\infty}P(X_t=i)\)，即马尔科夫链达到平稳分布的时候，处于第\(i\)个状态的概率。<br/> <strong>满足遍历定理的马尔可夫链</strong>：从任意起始点出发，最终都会收敛到同一个平稳分布，即殊途同归。<br/> 如果定义一个满足遍历定理的马尔可夫链，使得它的平稳分布等于目标分布\(p(x)\)，那么当经过足够长的游走时间\(m\)后，该链收敛，在之后的时间内每游走一次，就得到了服从目标分布的一个样本。该算法就被称为MCMC方法。<br/> 现在最大的问题就是：<strong>如何构造这么一个马尔可夫链，使得它的平稳分布等于目标分布？</strong><br/> TODO</li> <li><strong>基于MCMC的MLE方法</strong><br/> TODO <h3 id="score-based模型的解决方案">score-based模型的解决方案</h3> <p>对于一个基于能量函数的概率分布：<br/> \(p_\theta(x)=\frac{e^{-E_\theta(x)}}{Z_\theta}\)<br/> 对其对数似然求导：<br/> \(\begin{align} \nabla_\theta\log p_\theta(x) &amp;=\nabla_\theta\log\exp(-E_\theta(x))-\nabla_\theta\log Z_\theta\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))-\frac{1}{Z_\theta}\nabla_\theta Z_\theta &amp;\ ;\text{chain rule} \\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))-\frac{1}{Z_\theta}\nabla_\theta \int\exp(-E_\theta(x))\mathrm dx &amp;\ ;\text{definition of } Z_\theta\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))+\frac{1}{Z_\theta}\int\exp(-E_\theta(x))\nabla_\theta E_\theta(x)\mathrm dx &amp;\ ;\text{chain rule}\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))+\int\frac{\exp(-E_\theta(x))}{Z_\theta}\nabla_\theta E_\theta(x)\mathrm dx\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))+\mathbb{E}_{x\sim p_\theta(x)}\left[\nabla_\theta E_\theta(x)\right] &amp;\ ;\text{definition of } p_\theta(x)\\ \end{align}\)</p> </li> </ol> <p>基于score的生成模型和扩散模型非常相似，使用了score matching和Langevin dynamics技术进行生成。其中，</p> <ol> <li>score matching是估计目标分布的概率密度的梯度 （即score，分数），记\(p(x)\)是数据分布的概率密度函数，则这个分布的score被定义为\(\nabla_x\log p(x)\)，score matching则是训练一个网络\(s_\theta\)去近似score：<br/> \(\mathcal{E}_{p(x)}\left[ \Vert\nabla_x\log p(x)-s_\theta(x)\Vert^2_2 \right]=\int p(x)\Vert\nabla_x\log p(x)-s_\theta(x)\Vert^2_2 dx\)</li> <li>Langevin dynamics是使用score采样生成数据，采样方式如下：<br/> \(x_t=x_{t-1}+\frac{\delta}{2}\nabla_x\log p(x_{t-1})+\sqrt{\delta}\epsilon, \text{ where } \epsilon\sim\mathcal{N}(0, I)\) <h2 id="ddim">DDIM</h2> <h3 id="review-of-ddpm">Review of DDPM</h3> </li> <li> <p>Diffusion阶段<br/> \(\begin{align} q(x_t\vert x_0)&amp;=\boxed{\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)}\\ &amp;=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon \text{ ,where } \epsilon\sim\mathcal{N}(0,I) \end{align}\)</p> </li> <li>Reverse阶段<br/> 使用贝叶斯公式<br/> \(\begin{align} q(x_{t-1}\vert x_t)&amp;=\frac{q(x_t\vert x_{t-1})q(x_{t-1})}{q(x_t)} \end{align}\)<br/> 发现公式中\(q(x_{t-1})\)和\(q(x_t)\)不好求，根据DDPM的马尔科夫假设：<br/> \(\begin{align} q(x_{t-1}\vert x_t)&amp;=q(x_{t-1}\vert x_t,x_0)\\ &amp;=\frac{q(x_t\vert x_{t-1},x_0)q(x_{t-1}\vert x_0)}{q(x_t\vert x_0)}\\ &amp;=\frac{q(x_t\vert x_{t-1})q(x_{t-1}\vert x_0)}{q(x_t\vert x_0)}\\ &amp;=\boxed{\mathcal{N}(x_{t-1};\mu(x_t;\theta),\sigma_t^2I)} \end{align}\)<br/> 其中，\(\sigma_t\)可以用超参数表示，\(\mu(x_t;\theta)\)是一个神经网络，用于预测均值：<br/> \(\begin{align} \mu(x_t;\theta)&amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t+ \frac{\sqrt{\bar{x}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0\\ &amp;=\boxed{\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t+ \frac{\sqrt{\bar{x}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\hat{x}_{0\vert t}}\\ \sigma_t^2&amp;=\boxed{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t} \end{align}\) <h3 id="from-ddpm-to-ddim">From DDPM to DDIM</h3> </li> </ol> <p>同样是对分布\(q(x_{t-1}\vert x_t,x_0)\)进行求解：<br/> \(\begin{align} q(x_{t-1}\vert x_t,x_0) &amp;=\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon \text{ ,where }\epsilon\sim\mathcal{N}(0,I) \end{align}\)<br/> 在上式中，\(\epsilon\)是一个噪声，虽然可以重新从高斯分布采样，但是也可以使用噪声估计网络估计出来的结果\(\epsilon_\theta(x_t,t)\)：<br/> \(\begin{align} q(x_{t-1}\vert x_t,x_0) &amp;=\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon \text{ ,where }\epsilon\sim\mathcal{N}(0,I)\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t,t)\\ \end{align}\)<br/> 甚至可以同时考虑\(\epsilon\)和\(\epsilon_\theta(x_t,t)\)：<br/> \(\begin{align} q(x_{t-1}\vert x_t,x_0) &amp;=\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon \text{ ,where }\epsilon\sim\mathcal{N}(0,I)\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t,t)\\ \end{align}\)</p> <h1 id="应用篇">应用篇</h1> <h2 id="sr3">SR3</h2> <p>超分，训练数据是LR和SR配对的图片，以LR图片作为condition，生成SR图片</p> <h2 id="cdm">CDM</h2> <p>超分，级联的方式对小图进行超分，采用的方法就是SR3</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020230927200225-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020230927200225-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020230927200225-1400.webp"/> <img src="/assets/img/Pasted%20image%2020230927200225.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="sdedit">SDEdit</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020230927200246-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020230927200246-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020230927200246-1400.webp"/> <img src="/assets/img/Pasted%20image%2020230927200246.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>由于加噪过程是首先破坏高频信息，然后才破坏低频信息，所以加噪到一定程度之后，就就可以去掉不想要的细节纹理，但仍保留大体结构，于是生成出来的图像就既能遵循输入的引导，又显得真实。但是需要 realism-faithfulness trade-off</p> <h2 id="ilvr">ILVR</h2> <p>给定一个参考图像\(y\)，通过调整DDPM去噪过程，希望让模型生成的图像接近参考图像，作者定义的接近是让模型能够满足<br/> \(\phi_N(x_t)=\phi_N(y_t)\)<br/> \(\phi_N(\cdot)\)是一个低通滤波器（下采样之后再插值回来）。使用如下的算法：</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020230927201110-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020230927201110-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020230927201110-1400.webp"/> <img src="/assets/img/Pasted%20image%2020230927201110.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>即，对DDPM预测的\(x'_{t-1}\)加上bias：\(\phi_N(y_{t-1})-\phi_N(x'_{t-1})\)，可以证明，如果上/下采样采用的是最近邻插值，使用这种方法可以使得\(\phi_N(x_t)=\phi_N(y_t)\).<br/> 这种方法和classifier guidance很相似，甚至不需要训练一个外部模型，对算力友好。</p> <h2 id="diffusionclip">DiffusionCLIP</h2> <p>基于扩散模型的图像编辑，使用到的技术有DDIM Inversion，CLIP<br/> TODO</p> <h1 id="参考">参考</h1> <ol> <li><a href="https://www.bilibili.com/video/BV13P411J7dm">从零开始了解Diffusion Models</a></li> <li><a href="https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html">https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html</a></li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models</a></li> <li><a href="https://yang-song.net/blog/2021/score/">An introduction to Diffusion Probabilistic Models</a></li> <li><a href="https://www.zhihu.com/question/499485994/answer/2552791458">能量模型能做什么，和普通的神经网络模型有什么区别，为什么要用能量模型呢？</a></li> <li><a href="https://zhuanlan.zhihu.com/p/576779879">扩散模型与能量模型，Score-Matching和SDE，ODE的关系</a></li> <li><a href="https://zhuanlan.zhihu.com/p/250146007">你一定从未看过如此通俗易懂的马尔科夫链蒙特卡罗方法(MCMC)解读(上)</a></li> <li><a href="https://arxiv.org/pdf/2101.03288.pdf">How to Train Your Energy-Based Models</a></li> </ol>]]></content><author><name></name></author></entry><entry><title type="html">Diffusion Models - DDIM</title><link href="https://dw-dengwei.github.io/blog/Diffusion-Models-DDIM/" rel="alternate" type="text/html" title="Diffusion Models - DDIM"/><published>2023-10-06T00:00:00+00:00</published><updated>2023-10-06T00:00:00+00:00</updated><id>https://dw-dengwei.github.io/blog/Diffusion%20Models%20-%20DDIM</id><content type="html" xml:base="https://dw-dengwei.github.io/blog/Diffusion-Models-DDIM/"><![CDATA[<h1 id="review-of-ddpm">Review of DDPM</h1> <ol> <li> <p>Diffusion阶段<br/> \(\begin{align} q(x_t\vert x_0)&amp;=\boxed{\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)}\\ &amp;=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon \text{ ,where } \epsilon\sim\mathcal{N}(0,I) \end{align}\)</p> </li> <li> <p>Reverse阶段<br/> 使用贝叶斯公式<br/> \(\begin{align} q(x_{t-1}\vert x_t)&amp;=\frac{q(x_t\vert x_{t-1})q(x_{t-1})}{q(x_t)} \end{align}\)<br/> 发现公式中\(q(x_{t-1})\)和\(q(x_t)\)不好求，根据DDPM的马尔科夫假设：<br/> \(\begin{align} q(x_{t-1}\vert x_t)&amp;=q(x_{t-1}\vert x_t,x_0)\\ &amp;=\frac{q(x_t\vert x_{t-1},x_0)q(x_{t-1}\vert x_0)}{q(x_t\vert x_0)}\\ &amp;=\frac{q(x_t\vert x_{t-1})q(x_{t-1}\vert x_0)}{q(x_t\vert x_0)}\\ &amp;=\boxed{\mathcal{N}(x_{t-1};\mu(x_t;\theta),\sigma_t^2I)} \end{align}\)<br/> 其中，\(\sigma_t\)可以用超参数表示，\(\mu(x_t;\theta)\)是一个神经网络，用于预测均值：<br/> \(\begin{align} \mu(x_t;\theta)&amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t+ \frac{\sqrt{\bar{x}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0\\ &amp;=\boxed{\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t+ \frac{\sqrt{\bar{x}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\hat{x}_{0\vert t}}\\ \sigma_t^2&amp;=\boxed{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t} \end{align}\)</p> <h1 id="from-ddpm-to-ddim">From DDPM to DDIM</h1> <p>同样是对分布\(q(x_{t-1}\vert x_t,x_0)\)进行求解：<br/> \(\begin{align} q(x_{t-1}\vert x_t,x_0) &amp;=\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon \text{ ,where }\epsilon\sim\mathcal{N}(0,I) \end{align}\)<br/> 在上式中，\(\epsilon\)是一个噪声，虽然可以重新从高斯分布采样，但是也可以使用噪声估计网络估计出来的结果\(\epsilon_\theta(x_t,t)\)：<br/> \(\begin{align} q(x_{t-1}\vert x_t,x_0) &amp;=\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon \text{ ,where }\epsilon\sim\mathcal{N}(0,I)\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t,t)\\ \end{align}\)<br/> 甚至可以同时考虑\(\epsilon\)和\(\epsilon_\theta(x_t,t)\)：<br/> \(\begin{align} q(x_{t-1}\vert x_t,x_0) &amp;=\sqrt{\bar{\alpha}_{t-1}}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon \text{ ,where }\epsilon\sim\mathcal{N}(0,I)\\ &amp;=\sqrt{\bar{\alpha}_{t-1}} \hat{x}_{0\vert t} +\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t,t)\\ \end{align}\)</p> <h1 id="reference">Reference</h1> </li> </ol>]]></content><author><name>Wei Deng</name></author><category term="diffusion-model"/><summary type="html"><![CDATA[introduction about DDIM]]></summary></entry><entry><title type="html">Diffusion Models - DDPM</title><link href="https://dw-dengwei.github.io/blog/Diffusion-Models-DDPM/" rel="alternate" type="text/html" title="Diffusion Models - DDPM"/><published>2023-10-06T00:00:00+00:00</published><updated>2023-10-06T00:00:00+00:00</updated><id>https://dw-dengwei.github.io/blog/Diffusion%20Models%20-%20DDPM</id><content type="html" xml:base="https://dw-dengwei.github.io/blog/Diffusion-Models-DDPM/"><![CDATA[<h1 id="diffusion-process">Diffusion Process</h1> <p>前向扩散指的是将一个复杂分布转换成简单分布的过程\(\mathcal{T}:\mathbb{R}^d\mapsto\mathbb{R}^d\)，即：<br/> \(\mathbf{x}_0\sim p_\mathrm{complex}\Longrightarrow \mathcal{T}(\mathbf{x}_0)\sim p_\mathrm{prior}\)<br/> 在DDPM中，将这个过程定义为<strong>马尔可夫链</strong>，通过不断地向复杂分布中的样本\(x_0\sim p_\mathrm{complex}\)添加高斯噪声。这个加噪过程可以表示为\(q(\mathbf{x}_t\vert\mathbf{x}_{t-1})\)：<br/> \(\begin{align} q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) &amp;= \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})\\ \mathbf{x}_t&amp;=\sqrt{1-\beta_t}\mathbf{x}_{t-1}+\sqrt{\beta_t}\boldsymbol\epsilon \quad \boldsymbol\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}) \end{align}\)<br/> 其中，\(\{\beta_t\in(0,1)\}^T_{t=1}\)，是超参数。<br/> 从\(\mathbf{x}_0\)开始，不断地应用\(q(\mathbf{x}_t\vert\mathbf{x}_{t-1})\)，经过足够大的\(T\)步加噪之后，最终得到纯噪声\(\mathbf{x}_T\)：<br/> \(\mathbf{x}_0\sim p_\mathrm{complex}\rightarrow \mathbf{x}_1\rightarrow \cdots \mathbf{x}_t\rightarrow\cdots\rightarrow \mathbf{x}_T\sim p_\mathrm{prior}\)<br/> 除了迭代地使用\(q(\mathbf{x}_t\vert\mathbf{x}_{t-1})\)外，还可以使用\(q(\mathbf{x}_t\vert\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})\)一步到位，证明如下（两个高斯变量的线性组合仍然是高斯变量）：<br/> \(\begin{aligned} \mathbf{x}_t &amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} &amp;\ ;\alpha_t=1-\alpha_t\\ &amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} \\ &amp;= \dots \\ &amp;= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} &amp;\ ;\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0}, \mathbf{I}),\bar{\alpha}_t=\prod_{i=1}^t \alpha_i\ \end{aligned}\)<br/> 一般来说，超参数\(\beta_t\)的设置满足\(0&lt;\beta_1&lt;\cdots&lt;\beta_T&lt;1\)，则\(\bar{\alpha}_1 &gt; \cdots &gt; \bar{\alpha}_T\to1\)，则\(\mathbf{x}_T\)会只保留纯噪声部分。</p> <h1 id="reverse-process">Reverse Process</h1> <p>在前向扩散过程中，实现了：<br/> \(\mathbf{x}_0\sim p_\mathrm{complex}\rightarrow \mathbf{x}_1\rightarrow \cdots \mathbf{x}_t\rightarrow\cdots\rightarrow \mathbf{x}_T\sim p_\mathrm{prior}\)<br/> 如果能够实现将前向扩散过程反转，也就实现了从简单分布到复杂分布的映射。逆向扩散过程则是将前向过程反转，实现从简单分布随机采样样本，迭代地使用\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)，最终生成复杂分布的样本，即：<br/> \(\mathbf{x}_T\sim p_\mathrm{prior}\rightarrow \mathbf{x}_{T-1}\rightarrow \cdots \mathbf{x}_t\rightarrow\cdots\rightarrow \mathbf{x}_0\sim p_\mathrm{complex}\)<br/> 为了求取\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)，使用贝叶斯公式：<br/> \(\begin{align} q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)&amp;=\frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})q(\mathbf{x}_{t-1})}{q(\mathbf{x}_t)} \end{align}\)<br/> 然而，公式中\(q(x_{t-1})\)和\(q(x_t)\)不好求，根据DDPM的马尔科夫假设，可以为\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)添加条件（可以证明，如果向扩散过程中的\(\beta_t\)足够小，那么\(q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)\)是高斯分布。）：<br/> \(\begin{align} q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)&amp;=q(\mathbf{x}_{t-1}\vert\mathbf{x}_t,\mathbf{x}_0)\\ &amp;=\frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1},\mathbf{x}_0)q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)}{q(\mathbf{x}_t\vert\mathbf{x}_0)}\\ &amp;=\frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)}{q(\mathbf{x}_t\vert\mathbf{x}_0)}\\ &amp;=\mathcal{N}(\mathbf{x}_{t-1};\mu(\mathbf{x}_t;\theta),\sigma_t^2\mathbf I) \end{align}\)<br/> 其中，\(\mu(x_t;\theta)\)是高斯分布的均值，\(\sigma_t\)可以用超参数表示：<br/> \(\begin{align} \mu(\mathbf{x}_t;\theta)&amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0\\ \sigma_t&amp;=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t \end{align}\)<br/> 式中\(x_0\)可以反用公式\(\mathbf x_t=\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t\)：<br/> \(\mathbf x_0=\frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t\right)\)<br/> 则：<br/> \(\begin{align} \mu(\mathbf{x}_t;\theta)&amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0\\ &amp;=\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t+ \frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}\frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t\right)\\ &amp;=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\boldsymbol\epsilon_t\right) \end{align}\)<br/> 而在推理的时候，\(\boldsymbol\epsilon_t\)是未知的，所以使用神经网络进行预测。综上，逆向扩散过程：<br/> \(\begin{align} q(\mathbf{x}_{t-1}\vert\mathbf{x}_t)&amp;=\mathcal{N}(\mathbf{x}_{t-1};\mu(\mathbf{x}_t;\theta),\sigma_t^2\mathbf I)\\ &amp;=\mathcal{N}\left(\mathbf x_{t-1};\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\boldsymbol\epsilon_\theta(\mathbf x_t, t)\right),\left(\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\cdot\beta_t\right)^2\mathbf I\right)\\ \mathbf x_{t-1}&amp;=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\boldsymbol\epsilon_\theta(\mathbf x_t, t)\right)+\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\cdot\boldsymbol\epsilon\quad\boldsymbol\epsilon\sim\mathcal N(\mathbf 0, \mathbf I) \end{align}\)</p> <h1 id="training-object">Training Object</h1> <p>DDPM的训练目标是最小化训练数据的负对数似然：<br/> \(\begin{align} -\log p_\theta(\mathbf x_0) &amp;\le -\log p_\theta(\mathbf x_0) + \mathrm{KL}\left(q(\mathbf x_{1:T}\vert\mathbf x_0)\Vert p_\theta(\mathbf x_{1:T}\vert\mathbf x_0)\right) &amp;\ ;\mathrm{KL}(\cdot\Vert\cdot)\ge 0\\ &amp;=-\log p_\theta(\mathbf x_0)+\mathbb{E}_{\mathbf x_{1:T}\sim q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})/p_\theta(\mathbf x_0)}\right]&amp;\ ;p_\theta(\mathbf x_{1:T}\vert\mathbf x_0)=\frac{p_\theta(\mathbf x_{0:T})}{p_\theta(\mathbf x_0)}\\ &amp;=-\log p_\theta(\mathbf x_0)+\mathbb{E}_{\mathbf x_{1:T}\sim q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}+\log p_\theta(\mathbf x_0)\right]\\ &amp;=\mathbb{E}_{\mathbf x_{1:T}\sim q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}\right]\\ \end{align}\)<br/> 其中\(p_\theta(\mathbf x_{1:T}\vert\mathbf x_0)\)是使用网络估计分布\(q\)（变分推断），定义\(\mathcal{L}_{\mathrm{VLB}}\triangleq\mathbb{E}_q(\mathbf x_{0:T})\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}\right]\ge-\mathbb{E}_{q(\mathbf x_0)}\log p_\theta(\mathbf x_0)\)，那么VLB是训练数据的负对数似然的上节，最小化VLB就是最小化负对数似然。继续对VLB拆分：<br/> \(\begin{align} \mathcal{L}_{\mathrm{VLB}}&amp;=\mathbb{E}_{q(\mathbf x_{0:T})}\left[\log\frac{q(\mathbf x_{1:T}\vert\mathbf x_0)}{p_\theta(\mathbf x_{0:T})}\right]\\ &amp;=\mathbb{E}_q\left[\log\frac{\prod_{t=1}^{T}q(\mathbf x_t\vert\mathbf x_{t-1})}{p_\theta(\mathbf x_T)\prod_{t=1}^{T}p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}\right]\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=1}\log\frac{q(\mathbf x_t\vert\mathbf x_{t-1})}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}\right]\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=2}\log\frac{q(\mathbf x_t\vert\mathbf x_{t-1})}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}+\log\frac{q(\mathbf x_1\vert\mathbf x_0)}{p_\theta(\mathbf x_0\vert\mathbf x_1)}\right]\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=2}\log\frac{q(\mathbf x_t\vert\mathbf x_{t-1}, \mathbf x_0)}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}+\log\frac{q(\mathbf x_1\vert\mathbf x_0)}{p_\theta(\mathbf x_0\vert\mathbf x_1)}\right] &amp;\ ;q(\mathbf x_t\vert\mathbf x_{t-1})=q(\mathbf x_t\vert\mathbf x_{t-1}, \mathbf x_0)\\ &amp;=\mathbb{E}_q\left[-\log p_\theta(\mathbf x_T)+\sum\limits^{T}_{t=2}\log\left(\frac{q(\mathbf x_{t-1}\vert\mathbf x_{t}, \mathbf x_0)}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)} \frac{q(\mathbf x_t\vert\mathbf x_0)}{q(\mathbf x_{t-1}\vert\mathbf x_0)}\right)+\log\frac{q(\mathbf x_1\vert\mathbf x_0)}{p_\theta(\mathbf x_0\vert\mathbf x_1)}\right] &amp;\ ;\text{Bayes Theorem}\\ &amp;=\mathbb{E}_q\left[\log\frac{q(\mathbf x_T\vert\mathbf x_0)}{p_\theta(\mathbf x_T)}+\sum_{t=2}^{T}\log\frac{q(\mathbf x_{t-1}\vert\mathbf x_t, \mathbf x_0)}{p_\theta(\mathbf x_{t-1}\vert\mathbf x_t)}-\log p_\theta(\mathbf x_0\vert\mathbf x_1)\right]\\ &amp;=\mathbb{E}_q\left[\underbrace{\mathrm{KL}(q(\mathbf x_T\vert\mathbf x_0) \Vert p_\theta(\mathbf x_T))}_{\mathcal{L}_T} + \sum_{t=2}^{T}\underbrace{\mathrm{KL}(q(\mathbf x_{t-1}\vert\mathbf x_t, \mathbf x_0) \Vert p_\theta(\mathbf x_{t-1}\vert\mathbf x_t))}_{\mathcal{L}_{t-1}}-\underbrace{\log p_\theta(\mathbf x_0\vert\mathbf x_1)}_{\mathcal{L}_0}\right]\\ &amp;=\mathbb{E}_q\left[\mathcal{L}_T+\sum_{t=2}^{T}\mathcal{L}_{t-1}-\mathcal{L}_0\right] \end{align}\)</p> <ol> <li>由于\(\mathbf x_T\)是纯噪声，所以\(\mathcal{L}_T\)是常数</li> <li>对于\(\mathcal{L}_0\)，DDPM专门设计了特殊的\(p_\theta(\mathbf x_0\vert\mathbf x_1)\)</li> <li>对于\(\mathcal{L}_t\triangleq\mathrm{KL}(q(\mathbf x_t\vert\mathbf x_{t+1}, \mathbf x_0) \Vert p_\theta(\mathbf x_t \vert \mathbf x_{t+1})) \quad 1\le t \le T-1\)，是两个正态分布的KL散度，有解析解。在DDPM中，使用了简化之后的损失函数：<br/> \(\begin{align} \mathcal{L}_t^{\mathrm{simple}}&amp;=\mathbb{E}_{t\sim[1,T],\mathbf x_0,\boldsymbol\epsilon_t}\left[\Vert\boldsymbol\epsilon_t-\boldsymbol\epsilon_\theta(\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{1-\bar{\alpha}_t}\boldsymbol\epsilon_t,t)\Vert^2_2\right] \end{align}\) <h1 id="summary">Summary</h1> <p>综上，DDPM的训练和采样/推理过程如下图所示：</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231002142935-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231002142935-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231002142935-1400.webp"/> <img src="/assets/img/Pasted%20image%2020231002142935.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="reference">Reference</h1> <ol> <li><a href="https://www.bilibili.com/video/BV13P411J7dm">从零开始了解Diffusion Models</a></li> <li><a href="https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html">https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html</a></li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models</a></li> <li><a href="https://yang-song.net/blog/2021/score/">An introduction to Diffusion Probabilistic Models</a></li> </ol>]]></content><author><name>Wei Deng</name></author><category term="diffusion-model"/><summary type="html"><![CDATA[introduction about DDPM]]></summary></entry><entry><title type="html">Diffusion Models - Score-based Generative Models</title><link href="https://dw-dengwei.github.io/blog/Diffusion-Models-Score-based-Generative-Models/" rel="alternate" type="text/html" title="Diffusion Models - Score-based Generative Models"/><published>2023-10-06T00:00:00+00:00</published><updated>2023-10-06T00:00:00+00:00</updated><id>https://dw-dengwei.github.io/blog/Diffusion%20Models%20-%20Score-based%20Generative%20Models</id><content type="html" xml:base="https://dw-dengwei.github.io/blog/Diffusion-Models-Score-based-Generative-Models/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>score-based生成模型是一种新的生成模型范式，在score-based之前，主要存在两种类型的生成模型：</p> <ol> <li><strong>基于似然的生成模型</strong>：基于最大似然估计（MLE），使得从真实数据分布中采样出的数据能够在所建模的数据分布中概率最大化，即\(\max_\theta \mathbb{E}_{x\sim p_\mathrm{data}(x)}\left[\log p(x;\theta)\right]\)。这类模型通过最大化似然函数，直接学习数据集的分布，主要方法有VAE、流模型、能量模型</li> <li><strong>隐式生成模型</strong>：非直接学习数据集的分布，主要方法有GAN<br/> 它们分别具有以下限制：</li> <li><strong>对于基于似然的生成模型</strong>：对神经网络的结构要求高</li> <li><strong>对于隐式生成模型</strong>：训练不稳定、容易导致模式坍塌 <h1 id="preliminary">Preliminary</h1> <p>score-based生成模型就能很好地避免这些限制，在介绍score-based生成模型之前，需要明确几个概念：</p> </li> <li><strong>能量函数</strong><br/> 对于大多数分布，可以使用概率密度函数（PDF）进行描述，也可以使用能量函数进行描述：<br/> \(p(x)=\frac{e^{-E(x)}}{Z}\)<br/> 其中\(p(x)\)是PDF，\(E(x)\)是能量函数，\(Z=\int e^{-E(x)}\mathrm{d}x\)是归一化因子。以高斯分布为例，可以使用以下能量函数进行描述：<br/> \(\begin{align} E(x;\mu,\sigma^2)&amp;=\frac{1}{2\sigma^2}(x-\mu)^2\\ p(x)&amp;=\frac{e^{-E(x)}}{\int e^{-E(x)}\mathrm dx}=\frac{e^{\frac{1}{2\sigma^2}(x-\mu)^2}}{\sqrt{2\pi\sigma^2}} \end{align}\)</li> <li><strong>能量模型</strong><br/> 能量模型是基于能量函数的生成模型：<br/> \(p_\theta(x)=\frac{\exp(-E_\theta(x))}{Z_\theta}\)<br/> 基于能量函数对数据分布进行建模的时候，<strong>如何计算能量模型的归一化函数\(Z_\theta\)是一个较难的问题</strong>。传统的基于似然的生成模型（如自回归模型、流模型、VAE）都有自己的解决方式，但是都对能量模型做了太多的约束，各自都有其限制。</li> <li><strong>蒙特卡罗采样方法：拒绝-接受法</strong><br/> 目的：希望从一个复杂分布\(p(x)\)采样\(N\)个样本<br/> 方法：使用一个简单分布\(q(x)\)为媒介（例如：高斯分布），这个分布必须满足它的\(c&gt;0\)倍大于等于\(p(x)\)。首先从简单分布\(q(x)\)中采样得到\(x^*\)，然后以\(\frac{p(x^*)}{cq(x^*)}\)的概率保留这个样本，直到得到\(N\)个样本结束。</li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020231002164638-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020231002164638-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020231002164638-1400.webp"/> <img src="/assets/img/Pasted%20image%2020231002164638.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li><strong>MCMC</strong><br/> <strong>MCMC方法可以从复杂分布中进行采样</strong><br/> <strong>符号定义</strong>：\(\pi_i\triangleq\pi(i)=\lim_{t\to +\infty}P(X_t=i)\)，即马尔科夫链达到平稳分布的时候，处于第\(i\)个状态的概率。<br/> <strong>满足遍历定理的马尔可夫链</strong>：从任意起始点出发，最终都会收敛到同一个平稳分布，即殊途同归。<br/> 如果定义一个满足遍历定理的马尔可夫链，使得它的平稳分布等于目标分布\(p(x)\)，那么当经过足够长的游走时间\(m\)后，该链收敛，在之后的时间内每游走一次，就得到了服从目标分布的一个样本。该算法就被称为MCMC方法。<br/> 现在最大的问题就是：<strong>如何构造这么一个马尔可夫链，使得它的平稳分布等于目标分布？</strong><br/> TODO</li> <li><strong>基于MCMC的MLE方法</strong><br/> TODO <h1 id="how-score-based-generative-model-work">How Score-based Generative Model Work</h1> <p>对于一个基于能量函数的概率分布：<br/> \(p_\theta(x)=\frac{e^{-E_\theta(x)}}{Z_\theta}\)<br/> 对其对数似然求导：<br/> \(\begin{align} \nabla_\theta\log p_\theta(x) &amp;=\nabla_\theta\log\exp(-E_\theta(x))-\nabla_\theta\log Z_\theta\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))-\frac{1}{Z_\theta}\nabla_\theta Z_\theta &amp;\ ;\text{chain rule} \\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))-\frac{1}{Z_\theta}\nabla_\theta \int\exp(-E_\theta(x))\mathrm dx &amp;\ ;\text{definition of } Z_\theta\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))+\frac{1}{Z_\theta}\int\exp(-E_\theta(x))\nabla_\theta E_\theta(x)\mathrm dx &amp;\ ;\text{chain rule}\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))+\int\frac{\exp(-E_\theta(x))}{Z_\theta}\nabla_\theta E_\theta(x)\mathrm dx\\ &amp;=-\nabla_\theta\log\exp(E_\theta(x))+\mathbb{E}_{x\sim p_\theta(x)}\left[\nabla_\theta E_\theta(x)\right] &amp;\ ;\text{definition of } p_\theta(x)\\ \end{align}\)</p> </li> </ol> <p>基于score的生成模型和扩散模型非常相似，使用了score matching和Langevin dynamics技术进行生成。其中，</p> <ol> <li>score matching是估计目标分布的概率密度的梯度 （即score，分数），记\(p(x)\)是数据分布的概率密度函数，则这个分布的score被定义为\(\nabla_x\log p(x)\)，score matching则是训练一个网络\(s_\theta\)去近似score：<br/> \(\mathcal{E}_{p(x)}\left[ \Vert\nabla_x\log p(x)-s_\theta(x)\Vert^2_2 \right]=\int p(x)\Vert\nabla_x\log p(x)-s_\theta(x)\Vert^2_2 dx\)</li> <li>Langevin dynamics是使用score采样生成数据，采样方式如下：<br/> \(x_t=x_{t-1}+\frac{\delta}{2}\nabla_x\log p(x_{t-1})+\sqrt{\delta}\epsilon, \text{ where } \epsilon\sim\mathcal{N}(0, I)\) <h1 id="reference">Reference</h1> </li> <li><a href="https://www.zhihu.com/question/499485994/answer/2552791458">能量模型能做什么，和普通的神经网络模型有什么区别，为什么要用能量模型呢？</a></li> <li><a href="https://zhuanlan.zhihu.com/p/576779879">扩散模型与能量模型，Score-Matching和SDE，ODE的关系</a></li> <li><a href="https://zhuanlan.zhihu.com/p/250146007">你一定从未看过如此通俗易懂的马尔科夫链蒙特卡罗方法(MCMC)解读(上)</a></li> <li><a href="https://arxiv.org/pdf/2101.03288.pdf">How to Train Your Energy-Based Models</a></li> </ol>]]></content><author><name>Wei Deng</name></author><category term="diffusion-model"/><summary type="html"><![CDATA[introduction about score-based generative models]]></summary></entry><entry><title type="html">Diffusion Models - Application</title><link href="https://dw-dengwei.github.io/blog/Diffusion-Models-Application/" rel="alternate" type="text/html" title="Diffusion Models - Application"/><published>2023-10-04T15:09:00+00:00</published><updated>2023-10-04T15:09:00+00:00</updated><id>https://dw-dengwei.github.io/blog/Diffusion%20Models%20-%20Application</id><content type="html" xml:base="https://dw-dengwei.github.io/blog/Diffusion-Models-Application/"><![CDATA[<h1 id="sr3">SR3</h1> <p>超分，训练数据是LR和SR配对的图片，以LR图片作为condition，生成SR图片</p> <h1 id="cdm">CDM</h1> <p>超分，级联的方式对小图进行超分，采用的方法就是SR3</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020230927200225-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020230927200225-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020230927200225-1400.webp"/> <img src="/assets/img/Pasted%20image%2020230927200225.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="sdedit">SDEdit</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020230927200246-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020230927200246-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020230927200246-1400.webp"/> <img src="/assets/img/Pasted%20image%2020230927200246.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>由于加噪过程是首先破坏高频信息，然后才破坏低频信息，所以加噪到一定程度之后，就就可以去掉不想要的细节纹理，但仍保留大体结构，于是生成出来的图像就既能遵循输入的引导，又显得真实。但是需要 realism-faithfulness trade-off</p> <h1 id="ilvr">ILVR</h1> <p>给定一个参考图像\(y\)，通过调整DDPM去噪过程，希望让模型生成的图像接近参考图像，作者定义的接近是让模型能够满足<br/> \(\phi_N(x_t)=\phi_N(y_t)\)<br/> \(\phi_N(\cdot)\)是一个低通滤波器（下采样之后再插值回来）。使用如下的算法：</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Pasted%20image%2020230927201110-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Pasted%20image%2020230927201110-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Pasted%20image%2020230927201110-1400.webp"/> <img src="/assets/img/Pasted%20image%2020230927201110.png" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>即，对DDPM预测的\(x'_{t-1}\)加上bias：\(\phi_N(y_{t-1})-\phi_N(x'_{t-1})\)，可以证明，如果上/下采样采用的是最近邻插值，使用这种方法可以使得\(\phi_N(x_t)=\phi_N(y_t)\).<br/> 这种方法和classifier guidance很相似，甚至不需要训练一个外部模型，对算力友好。</p> <h1 id="diffusionclip">DiffusionCLIP</h1> <p>基于扩散模型的图像编辑，使用到的技术有DDIM Inversion，CLIP</p> <h1 id="reference">Reference</h1>]]></content><author><name>Wei Deng</name></author><category term="diffusion-model"/><summary type="html"><![CDATA[introduction about the applications of diffusion models]]></summary></entry></feed>